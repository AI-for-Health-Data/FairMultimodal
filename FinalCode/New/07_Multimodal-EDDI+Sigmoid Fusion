import os
import time
import random
import argparse
import numpy as np
import pandas as pd
from tqdm import tqdm

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import AdamW
from torch.optim.lr_scheduler import ReduceLROnPlateau
from torch.utils.data import TensorDataset, DataLoader

from transformers import BertModel, BertConfig, AutoTokenizer
from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, recall_score, precision_score

DEBUG = True

class FocalLoss(nn.Module):
    def __init__(self, gamma=2, alpha=None, reduction='mean', pos_weight=None):
        super(FocalLoss, self).__init__()
        self.gamma = gamma
        self.alpha = alpha
        self.reduction = reduction
        self.pos_weight = pos_weight

    def forward(self, logits, targets):
        bce_loss = F.binary_cross_entropy_with_logits(
            logits, targets, reduction='none', pos_weight=self.pos_weight
        )
        pt = torch.exp(-bce_loss)
        focal_loss = ((1 - pt) ** self.gamma) * bce_loss
        if self.alpha is not None:
            focal_loss = self.alpha * focal_loss
        if self.reduction == 'mean':
            return focal_loss.mean()
        elif self.reduction == 'sum':
            return focal_loss.sum()
        else:
            return focal_loss

def get_pos_weight(labels_series, device, clip_max=10.0):
    positive = labels_series.sum()
    negative = len(labels_series) - positive
    if positive == 0:
        weight = torch.tensor(1.0, dtype=torch.float, device=device)
    else:
        w = negative / positive
        w = min(w, clip_max)
        weight = torch.tensor(w, dtype=torch.float, device=device)
    if DEBUG:
        print("Positive weight:", weight.item())
    return weight

def get_age_bucket(age):
    if 15 <= age <= 29:
        return "15-29"
    elif 30 <= age <= 49:
        return "30-49"
    elif 50 <= age <= 69:
        return "50-69"
    elif 70 <= age <= 90:
        return "70-90"
    else:
        return "Other"

def map_ethnicity(e):
    mapping = {0: "White", 1: "Black", 2: "Hispanic", 3: "Asian"}
    return mapping.get(e, "Other")

def map_insurance(i):
    mapping = {0: "Government", 1: "Medicare", 2: "Medicaid", 3: "Private", 4: "Self Pay"}
    return mapping.get(i, "Other")

def compute_eddi(y_true, y_pred, sensitive_labels):
    """
    Computes subgroup-level EDDI and returns overall fairness metric.
    """
    unique_groups = np.unique(sensitive_labels)
    subgroup_eddi = {}
    overall_error = np.mean(y_pred != y_true)
    denom = max(overall_error, 1 - overall_error) if overall_error not in [0, 1] else 1.0
    for group in unique_groups:
        mask = (sensitive_labels == group)
        if np.sum(mask) == 0:
            subgroup_eddi[group] = np.nan
        else:
            er_group = np.mean(y_pred[mask] != y_true[mask])
            subgroup_eddi[group] = (er_group - overall_error) / denom
    eddi_attr = np.sqrt(np.sum(np.array(list(subgroup_eddi.values()))**2)) / len(unique_groups)
    return eddi_attr, subgroup_eddi

# BioClinicalBERT Fine-Tuning for clinical notes
class BioClinicalBERT_FT(nn.Module):
    def __init__(self, base_model, config, device):
        super(BioClinicalBERT_FT, self).__init__()
        self.BioBert = base_model
        self.device = device

    def forward(self, input_ids, attention_mask):
        outputs = self.BioBert(input_ids=input_ids, attention_mask=attention_mask)
        # Use the CLS token embedding
        cls_embedding = outputs.last_hidden_state[:, 0, :]
        return cls_embedding

def apply_bioclinicalbert_on_patient_notes(df, note_columns, tokenizer, model, device, aggregation="mean"):
    patient_ids = df["subject_id"].unique()
    aggregated_embeddings = []
    for pid in tqdm(patient_ids, desc="Aggregating text embeddings"):
        patient_data = df[df["subject_id"] == pid]
        notes = []
        for col in note_columns:
            vals = patient_data[col].dropna().tolist()
            notes.extend([v for v in vals if isinstance(v, str) and v.strip() != ""])
        if len(notes) == 0:
            aggregated_embeddings.append(np.zeros(model.BioBert.config.hidden_size))
        else:
            embeddings = []
            for note in notes:
                encoded = tokenizer.encode_plus(
                    text=note,
                    add_special_tokens=True,
                    max_length=128,
                    padding='max_length',
                    truncation=True,
                    return_attention_mask=True,
                    return_tensors='pt'
                )
                input_ids = encoded['input_ids'].to(device)
                attn_mask = encoded['attention_mask'].to(device)
                with torch.no_grad():
                    emb = model(input_ids, attn_mask)
                embeddings.append(emb.cpu().numpy())
            embeddings = np.vstack(embeddings)
            agg_emb = np.mean(embeddings, axis=0) if aggregation == "mean" else np.max(embeddings, axis=0)
            aggregated_embeddings.append(agg_emb)
    aggregated_embeddings = np.vstack(aggregated_embeddings)
    return aggregated_embeddings

# BEHRT Model for Demographic (Structured) Data
class BEHRTModel_Demo(nn.Module):
    def __init__(self, num_ages, num_genders, num_ethnicities, num_insurances, hidden_size=768):
        super(BEHRTModel_Demo, self).__init__()
        # The vocabulary size is a sum of unique values in each categorical variable plus 2 extra tokens.
        vocab_size = num_ages + num_genders + num_ethnicities + num_insurances + 2
        config = BertConfig(
            vocab_size=vocab_size,
            hidden_size=hidden_size,
            num_hidden_layers=6,
            num_attention_heads=6,
            intermediate_size=3072,
            max_position_embeddings=128,
            type_vocab_size=2,
            hidden_dropout_prob=0.1,
            attention_probs_dropout_prob=0.1
        )
        self.bert = BertModel(config)
        self.age_embedding = nn.Embedding(num_ages, hidden_size)
        self.gender_embedding = nn.Embedding(num_genders, hidden_size)
        self.ethnicity_embedding = nn.Embedding(num_ethnicities, hidden_size)
        self.insurance_embedding = nn.Embedding(num_insurances, hidden_size)

    def forward(self, input_ids, attention_mask, age_ids, gender_ids, ethnicity_ids, insurance_ids):
        # Clamp the indices to be within the embedding range.
        age_ids = torch.clamp(age_ids, 0, self.age_embedding.num_embeddings - 1)
        gender_ids = torch.clamp(gender_ids, 0, self.gender_embedding.num_embeddings - 1)
        ethnicity_ids = torch.clamp(ethnicity_ids, 0, self.ethnicity_embedding.num_embeddings - 1)
        insurance_ids = torch.clamp(insurance_ids, 0, self.insurance_embedding.num_embeddings - 1)
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        cls_token = outputs.last_hidden_state[:, 0, :]
        age_embeds = self.age_embedding(age_ids)
        gender_embeds = self.gender_embedding(gender_ids)
        eth_embeds = self.ethnicity_embedding(ethnicity_ids)
        ins_embeds = self.insurance_embedding(insurance_ids)
        extra = (age_embeds + gender_embeds + eth_embeds + ins_embeds) / 4.0
        demo_embedding = cls_token + extra
        return demo_embedding

# BEHRT Model for Lab (Structured) Data
class BEHRTModel_Lab(nn.Module):
    def __init__(self, lab_token_count, hidden_size=768, nhead=8, num_layers=2):
        super(BEHRTModel_Lab, self).__init__()
        self.hidden_size = hidden_size
        self.token_embedding = nn.Linear(1, hidden_size)
        # Positional embeddings for each lab feature token.
        self.pos_embedding = nn.Parameter(torch.randn(lab_token_count, hidden_size))
        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_size, nhead=nhead)
        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)

    def forward(self, lab_features):
        # lab_features is assumed to be of shape (batch, num_lab_features)
        x = lab_features.unsqueeze(-1)  # shape: (batch, num_lab_features, 1)
        x = self.token_embedding(x)     # shape: (batch, num_lab_features, hidden_size)
        x = x + self.pos_embedding.unsqueeze(0)  # broadcast positional embeddings
        x = x.permute(1, 0, 2)  # (num_lab_features, batch, hidden_size)
        x = self.transformer_encoder(x)
        x = x.permute(1, 0, 2)  # (batch, num_lab_features, hidden_size)
        lab_embedding = x.mean(dim=1)
        return lab_embedding

# Updated Multimodal Fusion Model with Dynamic EDDI-based Weighting

class MultimodalTransformer_EDDI_Sigmoid(nn.Module):
    def __init__(self, text_embed_size, behrt_demo, behrt_lab, device, fusion_hidden=512):
        """
        Modified fusion model with dynamic EDDI-based weighting.
        """
        super(MultimodalTransformer_EDDI_Sigmoid, self).__init__()
        self.behrt_demo = behrt_demo
        self.behrt_lab = behrt_lab
        self.device = device

        # Unimodal projectors: project each modality into a 256-dimensional space.
        self.demo_projector = nn.Sequential(
            nn.Linear(behrt_demo.bert.config.hidden_size, 256),
            nn.ReLU()
        )
        self.lab_projector = nn.Sequential(
            nn.Linear(behrt_lab.hidden_size, 256),
            nn.ReLU()
        )
        self.text_projector = nn.Sequential(
            nn.Linear(text_embed_size, 256),
            nn.ReLU()
        )
        
        # Sigmoid weights per modality for feature selection (one per modality)
        self.sig_weights_demo = nn.Parameter(torch.randn(256))
        self.sig_weights_lab = nn.Parameter(torch.randn(256))
        self.sig_weights_text = nn.Parameter(torch.randn(256))
        
        # Dynamic scalar weights for each modality (initialized equally)
        self.register_buffer('dynamic_weight_demo', torch.tensor(1/3))
        self.register_buffer('dynamic_weight_lab', torch.tensor(1/3))
        self.register_buffer('dynamic_weight_text', torch.tensor(1/3))
        
        # Modality-specific classifiers (used to compute EDDI for each modality)
        self.classifier_demo = nn.Linear(256, 3)
        self.classifier_lab = nn.Linear(256, 3)
        self.classifier_text = nn.Linear(256, 3)
        
        # Fusion MLP for final prediction.
        self.fusion_mlp = nn.Sequential(
            nn.Linear(256, fusion_hidden),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(fusion_hidden, 3)
        )

    def forward(self, demo_dummy_ids, demo_attn_mask,
                age_ids, gender_ids, ethnicity_ids, insurance_ids,
                lab_features, aggregated_text_embedding,
                return_modality_logits=False):
        # Get unimodal embeddings.
        demo_embedding = self.behrt_demo(demo_dummy_ids, demo_attn_mask,
                                         age_ids, gender_ids, ethnicity_ids, insurance_ids)
        lab_embedding = self.behrt_lab(lab_features)
        text_embedding = aggregated_text_embedding

        # Project each modality to 256 dimensions.
        demo_proj = self.demo_projector(demo_embedding)
        lab_proj = self.lab_projector(lab_embedding)
        text_proj = self.text_projector(text_embedding)

        # Apply sigmoid-based feature selection separately.
        demo_adj = demo_proj * torch.sigmoid(self.sig_weights_demo)
        lab_adj = lab_proj * torch.sigmoid(self.sig_weights_lab)
        text_adj = text_proj * torch.sigmoid(self.sig_weights_text)

        # Fuse modalities using dynamic scalar weights.
        w_demo = self.dynamic_weight_demo
        w_lab = self.dynamic_weight_lab
        w_text = self.dynamic_weight_text
        sum_w = w_demo + w_lab + w_text
        fused = (w_demo * demo_adj + w_lab * lab_adj + w_text * text_adj) / sum_w

        # Final fused prediction.
        fused_logits = self.fusion_mlp(fused)
        
        if return_modality_logits:
            # Also return modality-specific predictions (e.g., for the mortality outcome).
            logits_demo = self.classifier_demo(demo_adj)
            logits_lab = self.classifier_lab(lab_adj)
            logits_text = self.classifier_text(text_adj)
            modality_logits = {'demo': logits_demo, 'lab': logits_lab, 'text': logits_text}
            return fused_logits, modality_logits, fused
        else:
            return fused_logits, fused


def train_step(model, dataloader, optimizer, device,
               criterion, lambda_edd=1.0, lambda_l1=1e-3, target=1.0,
               lr_weight=0.01, threshold=0.5):
    model.train()
    running_loss = 0.0
    for batch in dataloader:
        (demo_dummy_ids, demo_attn_mask,
         age_ids, gender_ids, ethnicity_ids, insurance_ids,
         lab_features,
         aggregated_text_embedding,
         labels) = [x.to(device) for x in batch]

        optimizer.zero_grad()
        # Get fused logits along with modality-specific logits.
        fused_logits, modality_logits, _ = model(
            demo_dummy_ids, demo_attn_mask,
            age_ids, gender_ids, ethnicity_ids, insurance_ids,
            lab_features, aggregated_text_embedding,
            return_modality_logits=True
        )
        # Compute the primary BCE loss on fused logits.
        bce_loss = criterion(fused_logits, labels)
        # Dummy EDDI loss (MSE between logits and a target) for overall fairness.
        eddi_loss = ((fused_logits - target) ** 2).mean()
        # L1 regularization on all sigmoid weights.
        l1_reg = lambda_l1 * (torch.norm(model.sig_weights_demo, 1) +
                              torch.norm(model.sig_weights_lab, 1) +
                              torch.norm(model.sig_weights_text, 1))
        loss = bce_loss + lambda_edd * eddi_loss + l1_reg

        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        
        
        # Compute EDDI for the mortality outcome (index 0) from modality-specific predictions.
        age_groups = np.array([get_age_bucket(a) for a in age_ids.cpu().numpy()])
        labels_np = labels[:, 0].cpu().numpy().astype(int)
        modality_eddi = {}
        for key in modality_logits:
            probs = torch.sigmoid(modality_logits[key][:, 0]).detach().cpu().numpy()
            preds = (probs > threshold).astype(int)
            eddi_val, _ = compute_eddi(labels_np, preds, age_groups)
            modality_eddi[key] = eddi_val
        
        max_eddi = max(modality_eddi.values())
        with torch.no_grad():
            new_w_demo = model.dynamic_weight_demo + lr_weight * (max_eddi - modality_eddi['demo'])
            new_w_lab  = model.dynamic_weight_lab  + lr_weight * (max_eddi - modality_eddi['lab'])
            new_w_text = model.dynamic_weight_text + lr_weight * (max_eddi - modality_eddi['text'])
            sum_new = new_w_demo + new_w_lab + new_w_text
            # Normalize so that the sum of weights is 1.
            model.dynamic_weight_demo.copy_(new_w_demo / sum_new)
            model.dynamic_weight_lab.copy_(new_w_lab / sum_new)
            model.dynamic_weight_text.copy_(new_w_text / sum_new)
        
        running_loss += loss.item()
    return running_loss

def evaluate_model_multi(model, dataloader, device, threshold=0.5, print_eddi=False):
    model.eval()
    all_logits = []
    all_labels = []
    all_age = []
    all_ethnicity = []
    all_insurance = []
    
    with torch.no_grad():
        for batch in dataloader:
            (demo_dummy_ids, demo_attn_mask,
             age_ids, gender_ids, ethnicity_ids, insurance_ids,
             lab_features,
             aggregated_text_embedding,
             labels) = [x.to(device) for x in batch]
            logits, _ = model(
                demo_dummy_ids, demo_attn_mask,
                age_ids, gender_ids, ethnicity_ids, insurance_ids,
                lab_features, aggregated_text_embedding
            )
            all_logits.append(logits.cpu())
            all_labels.append(labels.cpu())
            all_age.append(age_ids.cpu())
            all_ethnicity.append(ethnicity_ids.cpu())
            all_insurance.append(insurance_ids.cpu())
    
    all_logits = torch.cat(all_logits, dim=0)
    all_labels = torch.cat(all_labels, dim=0)

    metrics = {}
    outcome_names = ["mortality", "los", "mech_ventilation"]
    # Evaluate each outcome separately.
    for i, outcome in enumerate(outcome_names):
        probs = torch.sigmoid(all_logits[:, i]).numpy()
        labels_np = all_labels[:, i].numpy()
        preds = (probs > threshold).astype(int)
        try:
            aucroc = roc_auc_score(labels_np, probs)
        except Exception:
            aucroc = float('nan')
        try:
            auprc = average_precision_score(labels_np, probs)
        except Exception:
            auprc = float('nan')
        f1 = f1_score(labels_np, preds, zero_division=0)
        recall = recall_score(labels_np, preds, zero_division=0)
        precision = precision_score(labels_np, preds, zero_division=0)
        metrics[outcome] = {"aucroc": aucroc, "auprc": auprc, "f1": f1,
                            "recall": recall, "precision": precision}

    # Compute EDDI statistics for each outcome.
    if print_eddi:
        ages = torch.cat(all_age, dim=0).numpy().squeeze()
        ethnicities = torch.cat(all_ethnicity, dim=0).numpy().squeeze()
        insurances = torch.cat(all_insurance, dim=0).numpy().squeeze()
        age_groups = np.array([get_age_bucket(a) for a in ages])
        eth_groups = np.array([map_ethnicity(e) for e in ethnicities])
        ins_groups = np.array([map_insurance(i) for i in insurances])
        
        eddi_stats_all = {}
        for i, outcome in enumerate(outcome_names):
            labels_np = all_labels[:, i].numpy().astype(int)
            preds_outcome = (torch.sigmoid(all_logits[:, i]).numpy() > threshold).astype(int)
            overall_age, age_eddi_sub = compute_eddi(labels_np, preds_outcome, age_groups)
            overall_eth, eth_eddi_sub = compute_eddi(labels_np, preds_outcome, eth_groups)
            overall_ins, ins_eddi_sub = compute_eddi(labels_np, preds_outcome, ins_groups)
            total_eddi = np.sqrt((overall_age**2 + overall_eth**2 + overall_ins**2)) / 3
            eddi_stats_all[outcome] = {
                "age_eddi": overall_age,
                "age_subgroup_eddi": age_eddi_sub,
                "ethnicity_eddi": overall_eth,
                "ethnicity_subgroup_eddi": eth_eddi_sub,
                "insurance_eddi": overall_ins,
                "insurance_subgroup_eddi": ins_eddi_sub,
                "final_EDDI": total_eddi
            }
            print(f"\n--- Fairness (EDDI) Statistics for {outcome} ---")
            print("Final Overall EDDI:", total_eddi)
        metrics["eddi_stats"] = eddi_stats_all

    return metrics

def train_pipeline():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print("Using device:", device)

    structured_data = pd.read_csv('final_structured_common.csv', low_memory=False)
    unstructured_data = pd.read_csv("final_unstructured_common.csv", low_memory=False)
    print("\n--- Debug Info: Before Merge ---")
    print("Structured data shape:", structured_data.shape)
    print("Unstructured data shape:", unstructured_data.shape)
    
    unstructured_data.drop(
        columns=["short_term_mortality", "los_binary", "mechanical_ventilation", "age",
                 "GENDER", "ETHNICITY", "INSURANCE"],
        errors='ignore',
        inplace=True
    )
    merged_df = pd.merge(
        structured_data,
        unstructured_data,
        on=["subject_id", "hadm_id"],
        how="inner",
        suffixes=("_struct", "_unstruct")
    )
    if merged_df.empty:
        raise ValueError("Merged DataFrame is empty. Check your merge keys.")

    merged_df["short_term_mortality"] = merged_df["short_term_mortality"].astype(int)
    merged_df["los_binary"] = merged_df["los_binary"].astype(int)
    merged_df["mechanical_ventilation"] = merged_df["mechanical_ventilation"].astype(int)
    
    note_columns = [col for col in merged_df.columns if col.startswith("note_")]
    def has_valid_note(row):
        for col in note_columns:
            if pd.notnull(row[col]) and isinstance(row[col], str) and row[col].strip():
                return True
        return False
    df_filtered = merged_df[merged_df.apply(has_valid_note, axis=1)].copy()
    print("After filtering, number of rows:", len(df_filtered))

    # Ensure 'age' exists.
    if "age" not in df_filtered.columns:
        if "Age" in df_filtered.columns:
            df_filtered.rename(columns={"Age": "age"}, inplace=True)
        else:
            df_filtered["age"] = 0

    print("Computing aggregated text embeddings for each patient...")
    tokenizer = AutoTokenizer.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
    bioclinical_bert_base = BertModel.from_pretrained("emilyalsentzer/Bio_ClinicalBERT")
    bioclinical_bert_ft = BioClinicalBERT_FT(bioclinical_bert_base, bioclinical_bert_base.config, device).to(device)
    aggregated_text_embeddings_np = apply_bioclinicalbert_on_patient_notes(
        df_filtered, note_columns, tokenizer, bioclinical_bert_ft, device, aggregation="mean"
    )
    print("Aggregated text embeddings shape:", aggregated_text_embeddings_np.shape)
    aggregated_text_embeddings_t = torch.tensor(aggregated_text_embeddings_np, dtype=torch.float32)

    demographics_cols = ["age", "GENDER", "ETHNICITY", "INSURANCE"]
    for col in demographics_cols:
        if col not in df_filtered.columns:
            print(f"Column {col} not found; creating default values.")
            df_filtered[col] = 0
        elif df_filtered[col].dtype == object:
            df_filtered[col] = df_filtered[col].astype("category").cat.codes

    exclude_cols = set(["subject_id", "ROW_ID", "hadm_id", "ICUSTAY_ID",
                        "short_term_mortality", "los_binary", "mechanical_ventilation",
                        "age", "GENDER", "ETHNICITY", "INSURANCE"])
    lab_feature_columns = [col for col in df_filtered.columns 
                           if col not in exclude_cols and not col.startswith("note_") 
                           and pd.api.types.is_numeric_dtype(df_filtered[col])]
    print("Number of lab feature columns:", len(lab_feature_columns))
    df_filtered[lab_feature_columns] = df_filtered[lab_feature_columns].fillna(0)

    # Normalize lab features.
    lab_features_np = df_filtered[lab_feature_columns].values.astype(np.float32)
    lab_mean = np.mean(lab_features_np, axis=0)
    lab_std = np.std(lab_features_np, axis=0)
    lab_features_np = (lab_features_np - lab_mean) / (lab_std + 1e-6)

   
    num_samples = len(df_filtered)
    demo_dummy_ids = torch.zeros((num_samples, 1), dtype=torch.long)
    demo_attn_mask = torch.ones((num_samples, 1), dtype=torch.long)
    age_ids = torch.tensor(df_filtered["age"].values, dtype=torch.long)
    gender_ids = torch.tensor(df_filtered["GENDER"].values, dtype=torch.long)
    ethnicity_ids = torch.tensor(df_filtered["ETHNICITY"].values, dtype=torch.long)
    insurance_ids = torch.tensor(df_filtered["INSURANCE"].values, dtype=torch.long)

    lab_features_t = torch.tensor(lab_features_np, dtype=torch.float32)
    labels_np = df_filtered[["short_term_mortality", "los_binary", "mechanical_ventilation"]].values.astype(np.float32)
    labels = torch.tensor(labels_np, dtype=torch.float32)

    dataset = TensorDataset(
        demo_dummy_ids, demo_attn_mask,
        age_ids, gender_ids, ethnicity_ids, insurance_ids,
        lab_features_t,
        aggregated_text_embeddings_t,
        labels
    )
    dataloader = DataLoader(dataset, batch_size=16, shuffle=True)

    NUM_AGES = df_filtered["age"].nunique()
    NUM_GENDERS = df_filtered["GENDER"].nunique()
    NUM_ETHNICITIES = df_filtered["ETHNICITY"].nunique()
    NUM_INSURANCES = df_filtered["INSURANCE"].nunique()
    print("\n--- Demographics Hyperparameters ---")
    print("NUM_AGES:", NUM_AGES)
    print("NUM_GENDERS:", NUM_GENDERS)
    print("NUM_ETHNICITIES:", NUM_ETHNICITIES)
    print("NUM_INSURANCES:", NUM_INSURANCES)
    NUM_LAB_FEATURES = len(lab_feature_columns)
    print("NUM_LAB_FEATURES (tokens):", NUM_LAB_FEATURES)


    behrt_demo = BEHRTModel_Demo(
        num_ages=NUM_AGES,
        num_genders=NUM_GENDERS,
        num_ethnicities=NUM_ETHNICITIES,
        num_insurances=NUM_INSURANCES,
        hidden_size=768
    ).to(device)
    behrt_lab = BEHRTModel_Lab(
        lab_token_count=NUM_LAB_FEATURES,
        hidden_size=768,
        nhead=8,
        num_layers=2
    ).to(device)


    multimodal_model = MultimodalTransformer_EDDI_Sigmoid(
        text_embed_size=768, 
        behrt_demo=behrt_demo,
        behrt_lab=behrt_lab,
        device=device,
        fusion_hidden=512
    ).to(device)


    optimizer = torch.optim.Adam(multimodal_model.parameters(), lr=1e-5)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)
    
    pos_weight_mort = get_pos_weight(df_filtered["short_term_mortality"], device)
    pos_weight_los = get_pos_weight(df_filtered["los_binary"], device)
    pos_weight_mech = get_pos_weight(df_filtered["mechanical_ventilation"], device)
    pos_weight = torch.tensor([pos_weight_mort.item(), pos_weight_los.item(), pos_weight_mech.item()], device=device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)


    num_epochs = 20
    lambda_edd = 1.0   # weight for the EDDI (MSE) loss term
    lambda_l1 = 1e-3   # weight for L1 regularization on sigmoid weights
    for epoch in range(num_epochs):
        multimodal_model.train()
        running_loss = train_step(multimodal_model, dataloader, optimizer, device,
                                  criterion, lambda_edd=lambda_edd, lambda_l1=lambda_l1, target=1.0,
                                  lr_weight=0.01, threshold=0.5)
        epoch_loss = running_loss / len(dataloader)
        print(f"[Epoch {epoch+1}] Train Loss: {epoch_loss:.4f}")
        scheduler.step(epoch_loss)

    metrics = evaluate_model_multi(multimodal_model, dataloader, device, threshold=0.5, print_eddi=True)
    print("\n--- Final Evaluation Metrics ---")
    for outcome, metric_vals in metrics.items():
        if outcome == "eddi_stats":
            print("\nFairness (EDDI) Statistics:")
            print(metric_vals)
        else:
            print(f"\nOutcome: {outcome}")
            print("AUROC     : {:.4f}".format(metric_vals["aucroc"]))
            print("AUPRC     : {:.4f}".format(metric_vals["auprc"]))
            print("F1 Score  : {:.4f}".format(metric_vals["f1"]))
            print("Recall    : {:.4f}".format(metric_vals["recall"]))
            print("Precision : {:.4f}".format(metric_vals["precision"]))

    print("\nTraining complete.")

if __name__ == "__main__":
    train_pipeline()

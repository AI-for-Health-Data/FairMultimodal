{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03b817b5-99f9-4bb9-b299-b4072e54bba4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[torch] in /opt/conda/envs/rapids/lib/python3.10/site-packages (4.48.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (4.65.0)\n",
      "Requirement already satisfied: torch>=2.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (2.6.0)\n",
      "Requirement already satisfied: accelerate>=0.26.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: psutil in /opt/conda/envs/rapids/lib/python3.10/site-packages (from accelerate>=0.26.0->transformers[torch]) (5.9.5)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (3.1.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from torch>=2.0->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from requests->transformers[torch]) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.5.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/envs/rapids/lib/python3.10/site-packages (from jinja2->torch>=2.0->transformers[torch]) (2.1.3)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.10/site-packages (4.12.2)\n"
     ]
    }
   ],
   "source": [
    "!/opt/conda/envs/rapids/bin/python -m pip install transformers[torch]\n",
    "!pip install --upgrade typing_extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6e0be44-f21d-44fb-988f-6779ae8c2cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "After filtering, number of rows: 46091\n",
      "Computing aggregated text embeddings for each patient...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Aggregating text embeddings: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 46091/46091 [2:40:08<00:00,  4.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated text embeddings shape: (46091, 768)\n",
      "[Epoch 1] Train Loss: 0.3989\n",
      "Metrics at threshold=0.5 after epoch 1: {'aucroc': 0.9469224961352024, 'auprc': 0.7256341613759754, 'f1': 0.47859308671922385, 'recall': 0.928252175958598, 'precision': 0.3224119617615818}\n",
      "[Epoch 2] Train Loss: 0.3287\n",
      "Metrics at threshold=0.5 after epoch 2: {'aucroc': 0.9535491564688638, 'auprc': 0.7499799206606024, 'f1': 0.4532466804863018, 'recall': 0.951540813926135, 'precision': 0.2974702162082659}\n",
      "[Epoch 3] Train Loss: 0.3110\n",
      "Metrics at threshold=0.5 after epoch 3: {'aucroc': 0.9582834744091259, 'auprc': 0.7676505781935334, 'f1': 0.5352877307274702, 'recall': 0.9277816984239002, 'precision': 0.37615641392465426}\n",
      "[Epoch 4] Train Loss: 0.2971\n",
      "Metrics at threshold=0.5 after epoch 4: {'aucroc': 0.9622182307345972, 'auprc': 0.7851289705168459, 'f1': 0.4778356854248404, 'recall': 0.9597741707833451, 'precision': 0.3181038515515359}\n",
      "[Epoch 5] Train Loss: 0.2836\n",
      "Metrics at threshold=0.5 after epoch 5: {'aucroc': 0.9650606026565338, 'auprc': 0.7974308452521414, 'f1': 0.5368660511173557, 'recall': 0.9437779346036227, 'precision': 0.3751285647498831}\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from transformers import BertModel, BertConfig, AutoTokenizer, AutoModel\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, recall_score, precision_score\n",
    "\n",
    "# ----- Focal Loss Definition -----\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, reduction='mean', pos_weight=None):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce_loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction='none', pos_weight=self.pos_weight\n",
    "        )\n",
    "        pt = torch.exp(-bce_loss)\n",
    "        focal_loss = ((1 - pt) ** self.gamma) * bce_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            focal_loss = self.alpha * focal_loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return focal_loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return focal_loss.sum()\n",
    "        else:\n",
    "            return focal_loss\n",
    "\n",
    "# ----- Get Positive Weight for Loss -----\n",
    "def get_pos_weight(labels_series, device):\n",
    "    positive = labels_series.sum()\n",
    "    negative = len(labels_series) - positive\n",
    "    if positive == 0:\n",
    "        weight = torch.tensor(1.0, dtype=torch.float, device=device)\n",
    "    else:\n",
    "        weight = torch.tensor(negative / positive, dtype=torch.float, device=device)\n",
    "    return weight\n",
    "\n",
    "# ----- BioClinicalBERT Fine-Tuning Wrapper -----\n",
    "class BioClinicalBERT_FT(nn.Module):\n",
    "    def __init__(self, base_model, config, device):\n",
    "        super(BioClinicalBERT_FT, self).__init__()\n",
    "        self.BioBert = base_model\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.BioBert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "        return cls_embedding\n",
    "\n",
    "# ----- Function to Compute Aggregated Text Embeddings -----\n",
    "def apply_bioclinicalbert_on_patient_notes(df, note_columns, tokenizer, model, device, aggregation=\"mean\"):\n",
    "    \"\"\"\n",
    "    For each unique patient (by subject_id), extracts all non-null notes from the specified note columns,\n",
    "    tokenizes them, computes the CLS embedding via BioClinicalBERT, and aggregates them (mean or max).\n",
    "    \"\"\"\n",
    "    patient_ids = df[\"subject_id\"].unique()\n",
    "    aggregated_embeddings = []\n",
    "    for pid in tqdm(patient_ids, desc=\"Aggregating text embeddings\"):\n",
    "        patient_data = df[df[\"subject_id\"] == pid]\n",
    "        notes = []\n",
    "        for col in note_columns:\n",
    "            vals = patient_data[col].dropna().tolist()\n",
    "            notes.extend([v for v in vals if isinstance(v, str) and v.strip() != \"\"])\n",
    "        if len(notes) == 0:\n",
    "            aggregated_embeddings.append(np.zeros(model.BioBert.config.hidden_size))\n",
    "        else:\n",
    "            embeddings = []\n",
    "            for note in notes:\n",
    "                encoded = tokenizer.encode_plus(\n",
    "                    text=note,\n",
    "                    add_special_tokens=True,\n",
    "                    max_length=128,\n",
    "                    padding='max_length',\n",
    "                    truncation=True,\n",
    "                    return_attention_mask=True,\n",
    "                    return_tensors='pt'\n",
    "                )\n",
    "                input_ids = encoded['input_ids'].to(device)\n",
    "                attn_mask = encoded['attention_mask'].to(device)\n",
    "                with torch.no_grad():\n",
    "                    emb = model(input_ids, attn_mask)\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "            embeddings = np.vstack(embeddings)\n",
    "            agg_emb = np.mean(embeddings, axis=0) if aggregation == \"mean\" else np.max(embeddings, axis=0)\n",
    "            aggregated_embeddings.append(agg_emb)\n",
    "    aggregated_embeddings = np.vstack(aggregated_embeddings)\n",
    "    return aggregated_embeddings\n",
    "\n",
    "# ----- Evaluation Function for Mortality -----\n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_logits = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            aggregated_text_embedding, labels_mortality = [x.to(device) for x in batch]\n",
    "            logits = model(aggregated_text_embedding)\n",
    "            all_logits.append(logits.cpu())\n",
    "            all_labels.append(labels_mortality.cpu())\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0)\n",
    "    probs = torch.sigmoid(all_logits).numpy()\n",
    "    labels_np = all_labels.numpy()\n",
    "\n",
    "    try:\n",
    "        aucroc = roc_auc_score(labels_np, probs)\n",
    "    except Exception:\n",
    "        aucroc = float('nan')\n",
    "    try:\n",
    "        auprc = average_precision_score(labels_np, probs)\n",
    "    except Exception:\n",
    "        auprc = float('nan')\n",
    "    preds = (probs > threshold).astype(int)\n",
    "    f1 = f1_score(labels_np, preds, zero_division=0)\n",
    "    recall = recall_score(labels_np, preds, zero_division=0)\n",
    "    precision = precision_score(labels_np, preds, zero_division=0)\n",
    "    metrics = {\"aucroc\": aucroc, \"auprc\": auprc, \"f1\": f1, \"recall\": recall, \"precision\": precision}\n",
    "    return metrics, all_logits\n",
    "\n",
    "# ----- Text-based Mortality Model (Structure Similar to Fusion Code) -----\n",
    "class MultimodalTransformer(nn.Module):\n",
    "    def __init__(self, text_embed_size, hidden_size=512):\n",
    "        super(MultimodalTransformer, self).__init__()\n",
    "        self.text_projector = nn.Sequential(\n",
    "            nn.Linear(text_embed_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_size, 1)  # Single output for mortality\n",
    "        )\n",
    "\n",
    "    def forward(self, aggregated_text_embedding):\n",
    "        text_proj = self.text_projector(aggregated_text_embedding)\n",
    "        mortality_logits = self.classifier(text_proj)\n",
    "        return mortality_logits\n",
    "\n",
    "# ----- Training Step Function -----\n",
    "def train_step(model, dataloader, optimizer, device, criterion):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for batch in dataloader:\n",
    "        aggregated_text_embedding, labels_mortality = [x.to(device) for x in batch]\n",
    "        optimizer.zero_grad()\n",
    "        mortality_logits = model(aggregated_text_embedding)\n",
    "        # Updated loss: only mortality\n",
    "        loss = criterion(mortality_logits, labels_mortality.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    return running_loss\n",
    "\n",
    "# ----- Main Training Pipeline -----\n",
    "def train_pipeline():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Using device:\", device)\n",
    "\n",
    "    # Load unstructured data\n",
    "    df = pd.read_csv(\"final_unstructured.csv\", low_memory=False)\n",
    "\n",
    "    # Identify note columns (assumed to start with \"note_\")\n",
    "    note_columns = [col for col in df.columns if col.startswith(\"note_\")]\n",
    "    if len(note_columns) == 0:\n",
    "        raise ValueError(\"No note columns found in the data.\")\n",
    "\n",
    "    # Filter rows with valid note text\n",
    "    def has_valid_note(row):\n",
    "        for col in note_columns:\n",
    "            if pd.notnull(row[col]) and isinstance(row[col], str) and row[col].strip():\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    df_filtered = df[df.apply(has_valid_note, axis=1)].copy()\n",
    "    print(\"After filtering, number of rows:\", len(df_filtered))\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    if \"subject_id\" not in df_filtered.columns:\n",
    "        raise ValueError(\"Column 'subject_id' not found in data.\")\n",
    "    if \"short_term_mortality\" not in df_filtered.columns:\n",
    "        raise ValueError(\"Column 'short_term_mortality' not found in data.\")\n",
    "\n",
    "    # Initialize tokenizer and BioClinicalBERT for text embeddings\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    bioclinical_bert_base = BertModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    bioclinical_bert_ft = BioClinicalBERT_FT(bioclinical_bert_base, bioclinical_bert_base.config, device).to(device)\n",
    "\n",
    "    # Compute aggregated text embeddings per patient\n",
    "    print(\"Computing aggregated text embeddings for each patient...\")\n",
    "    aggregated_text_embeddings_np = apply_bioclinicalbert_on_patient_notes(\n",
    "        df_filtered, note_columns, tokenizer, bioclinical_bert_ft, device, aggregation=\"mean\"\n",
    "    )\n",
    "    print(\"Aggregated text embeddings shape:\", aggregated_text_embeddings_np.shape)\n",
    "\n",
    "    # Group by subject_id to obtain one label per patient (using first occurrence)\n",
    "    grouped = df_filtered.groupby(\"subject_id\", sort=False).first().reset_index()\n",
    "    unique_subject_ids = df_filtered[\"subject_id\"].unique()\n",
    "    grouped['order'] = pd.Categorical(grouped['subject_id'], categories=unique_subject_ids, ordered=True)\n",
    "    grouped = grouped.sort_values('order')\n",
    "\n",
    "    labels_mortality = torch.tensor(grouped[\"short_term_mortality\"].values, dtype=torch.float32)\n",
    "    aggregated_text_embeddings_t = torch.tensor(aggregated_text_embeddings_np, dtype=torch.float32)\n",
    "\n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(aggregated_text_embeddings_t, labels_mortality)\n",
    "    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "    # Compute positive weight for mortality\n",
    "    mortality_pos_weight = get_pos_weight(grouped[\"short_term_mortality\"], device)\n",
    "\n",
    "    # Initialize model, optimizer, scheduler, and loss criterion\n",
    "    model = MultimodalTransformer(text_embed_size=768, hidden_size=512).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2, verbose=True)\n",
    "    criterion = FocalLoss(gamma=2, pos_weight=mortality_pos_weight, reduction='mean')\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = train_step(model, dataloader, optimizer, device, criterion)\n",
    "        epoch_loss = train_loss / len(dataloader)\n",
    "        print(f\"[Epoch {epoch+1}] Train Loss: {epoch_loss:.4f}\")\n",
    "        scheduler.step(epoch_loss)\n",
    "        metrics, _ = evaluate_model(model, dataloader, device, threshold=0.5)\n",
    "        print(f\"Metrics at threshold=0.5 after epoch {epoch+1}: {metrics}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296664eb-6fca-453c-8b11-dde8e469dbb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
